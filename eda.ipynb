{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This file is a modified version of the original:\n# https://www.kaggle.com/code/burhanuddinlatsaheb/eda-visualizations-audio-exploration","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<b><font size = 2><span style=\"font-family:'Times New Roman';color:#03045e\">🐤EDA|🐦Visualizations + Audio Exploration 🔉🔉 </span></font></b>  \n\n<b><font size = 2><span style=\"font-family:'Times New Roman';color:#03045e;\">Created By Burhanuddin Latsaheb </span></font> </b> \n\n\n\n#  <center><font size = 8><span style=\"color:#0077b6;font-family:'Times New Roman'\">🐤EDA|🐦Visualizations + Audio Exploration 🔉🔉</span></font></center>\n\n\n <center><font size =4><span style=\"font-family:'Times New Roman';color:#03045e\"> If you find this notebook useful,support with an upvote👍👍 </span></font></center>\n","metadata":{}},{"cell_type":"markdown","source":"#  <center><font size = 3><span style=\"color:#38a3a5\"> <p style=\"background-color:#90e0ef;font-family:newtimeroman;color:#03045e;font-size:200%;text-align:center;border-radius:100px 10px;\">INTRODUCTION</p>   </span></font></center>\n \n<font size = 5><span style=\"color:#202833;font-family:'Times New Roman'\">Notebook Overview : </span></font>\n\n<!-- * <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\"> This notebook contains:  </span></font> -->\n1. <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">An  extensive <b>EDA</b> of the different Bird sounds and its different characteristics   </span></font>\n2. <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">The goal of this competition is to use machine learning to <b>identify Eastern African bird species by sound</b>.</span></font>\n\n\n<font size = 5><span style=\"color:#202833;font-family:'Times New Roman'\">Introduction to the BirdCLEF 2023 Competition:</span></font>\n\n<font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">\nIn this competition, participants are challenged to develop algorithms that can accurately identify bird species from audio recordings. The dataset consists of over 1 million audio recordings from around the world, with a total duration of over 1,000 hours. Each recording is labeled with the corresponding bird species, and participants are tasked with developing a machine learning model that can accurately classify the species in new, unseen recordings.</span></font>\n\n<font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">\nThe BirdCLEF 2023 competition provides a unique opportunity for researchers and data scientists to advance the field of bioacoustics and contribute to the preservation of bird populations around the world. </span></font>","metadata":{}},{"cell_type":"markdown","source":"<a id='top'></a>\n#  <center><font size = 3><span style=\"color:#38a3a5\"> <p style=\"background-color:#90e0ef;font-family:newtimeroman;color:#03045e;font-size:200%;text-align:center;border-radius:100px 10px;\">TABLE OF CONTENTS</p>   </span></font></center>\n- [1. Imports 📂](#1)\n- [2. EDA 📊](#2)\n    * [Train Meta Data Info](#2.1)\n    * [Plots](#2.2)\n    * [Correlation Plots](#2.4)\n    * [Interactive Map Plots](#2.3)\n       * [Scatter Plot](#2.3.1)\n       * [Map Box (Open Street Map)](#2.3.2)\n       * [Map Box(Terrain View)](#2.3.3)\n    * [EBird Taxonomy](#2.5)\n- [3. AUDIO EXPLORATION 🔉🔉](#3)\n    * [Helper Function](#3.1)\n    * [Audio Exploration](#3.2)\n        * [Black-and-white Mannikin](#3.2.1)\n        * [African Black-headed Oriole](#3.2.2)\n        * [African Bare-eyed Thrush](#3.2.3)\n        * [African Gray Flycatcher](#3.2.4)\n        * [African Goshawk](#3.2.5)\n","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n#  <center><font size = 3><span style=\"color:#a8dadc\"> <p style=\"background-color:#90e0ef;font-family:newtimeroman;color:#03045e;font-size:200%;text-align:center;border-radius:100px 10px;\">1. IMPORTS📂 </p>   </span></font></center>\n\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"import ast\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n#  <center><font size = 3><span style=\"color:#a8dadc\">  <p style=\"background-color:#90e0ef;font-family:newtimeroman;color:#03045e;font-size:200%;text-align:center;border-radius:100px 10px;\">2. 📊EDA 📊 </p>   </span></font></center>\n\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n     💡 &nbsp;<b><u>Observations in Train Meta Data:</u></b><br>\n * <i> There are total of <b><u>12</u></b> columns and <b><u>16941</u></b> rows in <b><u>train meta</u></b> data.</i><br>\n* <i> Train data contains <b><u>16941</u></b> values with <b><u>454(0.2%)</u></b> of  missing values.</i><br>\n* <i></i>There are total of <b><u>12</u></b> columns : <b><u>3</u></b> numerical , <b><u>9</u></b> categorical<br>\n* <i> <b><u>454 Missing </u></b> values in the <b><u>train meta</u></b> data.</i><br>\n</div>","metadata":{}},{"cell_type":"code","source":"trainmeta_df = pd.read_csv(\"/kaggle/input/birdclef-2023/train_metadata.csv\")\ntrainmeta_df.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = trainmeta_df[trainmeta_df[\"primary_label\"] == \"afgfly1\"]\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Train Meta Data Info</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n     💡💡 &nbsp;<b><u>Further Insights of train dataset</u></b><br>\n    * <i> <b><u>Latitude </u></b> &  <b><u>Longitude</u></b> have  <b>227(1.34%) Missing Values</b></i><br>\n    * <i> <b><u>Longitude</u></b> & <b><u>Rating</u></b> are <b><u>skewed</u></b></i><br>\n    * <i> <b><u>Primary labels</u></b> , <b><u>Secondary Labels</u></b> , <b><u>Type</u></b> , <b><u>Scientific Name</u></b> , <b><u>Common Name</u></b> , <b><u>Author</u></b> ,<b><u>Filename</u></b> have  <b><u>high Cardinality</u></b></i><br>\n</div>\n","metadata":{}},{"cell_type":"code","source":"print(trainmeta_df.info())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Duplicate values in train metadata","metadata":{}},{"cell_type":"code","source":"ool_idxs = []\n\nfor col in trainmeta_df.columns:\n    col_list = []\n    col_list.append(col)\n    if col != \"url\": col_list.append(\"url\")\n    if col != \"filename\": col_list.append(\"filename\")\n    train_metadata_dup = trainmeta_df.drop(columns = col_list)\n    dup_idx = list(train_metadata_dup.loc[train_metadata_dup.duplicated(keep = \"first\")].index)\n    print(f\"# of duplicates: {len(dup_idx)} --> columns dropped: {col_list}\")\n\n    if col == \"primary_label\":\n        ool_idxs.append(dup_idx)\n        \ntrainmeta_df.loc[sum(ool_idxs, [])][\"primary_label\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to find duplicate audio files - inefficient, use audio durations instead\ndef get_files(df, common_name):\n    temp_df = df.query(\"common_name == @common_name\")\n    sorted_files = sorted(list(temp_df[\"filename\"]))\n    file_list = [os.path.join(\"/kaggle/input/birdclef-2023/train_audio/\", file) for file in sorted_files]\n    return file_list\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_duplicate_audio(file1, file2):\n    \n    path = \"/kaggle/input/birdclef-2023/train_audio/\"\n    filename1 = path + file1\n    filename2 = path + file2\n\n    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n\n    y1, sr1 = librosa.load(filename1)\n    spec1 = librosa.display.specshow(librosa.power_to_db(librosa.feature.melspectrogram(y=y1, sr=sr1, n_mels=128, fmax=8000)), \n                                     y_axis='mel', fmax=8000, x_axis='time', ax=axs[0])\n    \n    axs[0].set_title(f'Mel-Spectrogram | file: {filename1.split(\"/\")[-1]}')\n    plt.colorbar(spec1, format='%+2.0f dB', ax=axs[0])\n\n    y2, sr2 = librosa.load(filename2)\n    spec2 = librosa.display.specshow(librosa.power_to_db(librosa.feature.melspectrogram(y=y2, sr=sr2, n_mels=128, fmax=8000)), \n                                     y_axis='mel', fmax=8000, x_axis='time', ax=axs[1])\n    \n    axs[1].set_title(f'Mel-Spectrogram | file: {filename2.split(\"/\")[-1]}')\n    plt.colorbar(spec2, format='%+2.0f dB', ax=axs[1])\n\n    print(\" \" * 5, filename1.split(\"audio\")[1])\n    display(Audio(filename1))\n    print(\" \" * 5, filename2.split(\"audio\")[1])\n    display(Audio(filename2))\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_duplicate_audio(\"combul2/XC748220.ogg\", \"combul2/XC748221.ogg\") # Common Bulbul","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Plots</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(trainmeta_df, x=\"primary_label\", nbins=len(trainmeta_df[\"primary_label\"].unique()))\nfig.update_layout(title_text=\"Distribution of Primary Labels\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"primary_label_counts = trainmeta_df.primary_label.value_counts().sort_values()\n\nfig = px.bar(\n    x=primary_label_counts.values, y=primary_label_counts.index,\n    color_discrete_sequence=['cornflowerblue'],\n    orientation='h', height=1000\n)\nfig.update_layout(xaxis_title=\"Count\", yaxis_title=\"Label\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"secondary_df = trainmeta_df['secondary_labels'].str.replace('[','').str.replace(']','').str.replace('\\'','').str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('secondary_label')\nsecondary_df = pd.merge(trainmeta_df.drop(columns=['secondary_labels']), secondary_df, left_index=True, right_index=True)\n\nfig = px.histogram(secondary_df, x=\"secondary_label\", title=\"Distribution of Secondary Labels\")\nfig.update_layout(xaxis_title=\"Secondary Label\", yaxis_title=\"Count\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the list of labels in the \"type\" column\nlabels = [label.strip(\"[]'\") for sublist in trainmeta_df['type'].apply(ast.literal_eval) for label in sublist]\n\n# Count the occurrence of each label\nlabel_counts = Counter(labels)\n\n# Create a bar plot of the label counts\nfig = px.bar(x=list(label_counts.keys()), y=list(label_counts.values()))\nfig.update_layout(title_text=\"Distribution of Types\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1 = px.box(trainmeta_df, y=\"latitude\")\nfig1.update_layout(title_text=\"Distribution of Latitude\",\n                   yaxis=dict(title=\"Latitude\"))\n\n# create a box plot for longitude\nfig2 = px.box(trainmeta_df, y=\"longitude\")\nfig2.update_layout(title_text=\"Distribution of Longitude\",\n                   yaxis=dict(title=\"Longitude\"))\n\n# create a box plot for rating\nfig3 = px.box(trainmeta_df, y=\"rating\")\nfig3.update_layout(title_text=\"Distribution of Ratings\",\n                   yaxis=dict(title=\"Rating\"))\n\n# show the figures\nfig1.show()\nfig2.show()\nfig3.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(trainmeta_df, x=\"scientific_name\", nbins=len(trainmeta_df[\"scientific_name\"].unique()))\nfig.update_layout(title_text=\"Distribution of Scientific Names\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = px.histogram(trainmeta_df, x=\"common_name\", nbins=len(trainmeta_df[\"common_name\"].unique()))\nfig.update_layout(title_text=\"Distribution of Common Names\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(trainmeta_df, x=\"author\", nbins=len(trainmeta_df[\"author\"].unique()))\nfig.update_layout(title_text=\"Distribution of Authors\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = px.histogram(trainmeta_df, x=\"rating\", nbins=len(trainmeta_df[\"rating\"].unique()) , color_discrete_sequence=['red'])\nfig.update_layout(title_text=\"Distribution of Ratings\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Audio Samples","metadata":{}},{"cell_type":"code","source":"# binning the dataset based on number of audio samples avaiable\nplt.figure(figsize=(10, 5)) # width, height (inches)\n\ntemp = trainmeta_df['primary_label'].value_counts()\nax = sns.histplot(temp, binwidth=100)\ntotals = []\nfor p in ax.patches:\n    totals.append(p.get_height())\ntotal = sum(totals)\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n    x = p.get_x() + (p.get_width() / 2)\n    y = p.get_y() + p.get_height() + 0.01\n    ax.annotate(percentage, (x, y))\n\nplt.xlabel('No of audio samples available for a bird spcies')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Audio Duration","metadata":{}},{"cell_type":"code","source":"import multiprocessing\nfrom tqdm.auto import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_path = '/kaggle/input/birdclef-2023/train_audio'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ref: https://stackoverflow.com/a/45276885/12828621\n\n#NOTE: without mutliprocessing this cells takes more than 1 hour to run, \n#but with mutltiprocesing it runs in less than 5 minutes\n\n# plot the length of audio recordings as a distribution\n# list of audio file paths\ntrainmeta_df['path'] = audio_path + '/' + trainmeta_df['filename']\naudio_files = trainmeta_df['path'].tolist()\n                                        \n# get the duration of each file in seconds (using multiprocessing) (we have in kaggle 4cores)\ndef calculate_audio_length(filepath):\n    y, sr = librosa.load(filepath, sr=None)\n    length = librosa.get_duration(y=y, sr=sr)\n    return length\n\nn_cores = multiprocessing.cpu_count()\nprint('Available cores: ', n_cores)\nwith multiprocessing.Pool(n_cores) as p:\n    duration = list(tqdm(p.imap(calculate_audio_length, audio_files), total=len(audio_files)))\n\n\n#plot\nsns.kdeplot(x=duration)\npercentile = np.percentile(duration, 99)\nprint('99 % of audio recordings is of length: ', round(percentile), ' seconds')\nplt.axvline(x=percentile, color='r', linestyle='--')\nplt.xlabel(\"Duration (s)\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Distribution of Audio Recording Durations.\\n 99% of audio recordings is of length: {round(percentile)} seconds\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Correlation Plots</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"# drop columns from correlation matrix\ncorr = trainmeta_df.corr()\n\n# create correlation heatmap\nfig = px.imshow(corr,\n                labels=dict(x=\"Columns\", y=\"Columns\", color=\"Correlation\"),\n                x=corr.columns,\n                y=corr.columns,\n                color_continuous_scale='RdBU',\n                zmin=-1,\n                zmax=1,\n                title=\"Correlation Heatmap\")\n                \n# add text annotations\nannotations = []\nfor i, row in enumerate(corr.values):\n    for j, value in enumerate(row):\n        text = '{:.2f}'.format(value)\n        annotations.append(dict(x=corr.columns[j], y=corr.columns[i], text=text, showarrow=False))\n\nfig.update_layout(width=800, height=800)\nfig.update_traces(showscale=True, colorbar_thickness=25, colorbar_len=0.75)\nfig.update_layout(margin=dict(l=50, r=50, b=100, t=100, pad=4))\nfig.update_layout(annotations=annotations)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Interactive Map Plots</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3.1\"></a>\n<p style=\"background-color:#f0f3bd;font-family:newtimeroman;color:#05668d;font-size:140%;text-align:center;border-radius:200px 10px;\">Scatter Plot</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(trainmeta_df, x=\"longitude\", y=\"latitude\", color=\"common_name\")\nfig.update_layout(title=\"Distribution of Recordings by Location\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.2\"></a>\n<p style=\"background-color:#f0f3bd;font-family:newtimeroman;color:#05668d;font-size:140%;text-align:center;border-radius:200px 10px;\">Map Box (Open Street Map)</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"# fig = px.scatter_mapbox(trainmeta_df, lat=\"latitude\", lon=\"longitude\", color=\"common_name\",\n#                         hover_name=\"filename\", hover_data=[\"common_name\", \"author\", \"rating\"],\n#                         zoom=3, height=500)\n# fig.update_layout(mapbox_style=\"open-street-map\")\n# fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n# fig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.3\"></a>\n<p style=\"background-color:#f0f3bd;font-family:newtimeroman;color:#05668d;font-size:140%;text-align:center;border-radius:200px 10px;\">Map Box(Terrain View)</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"# fig = px.density_mapbox(trainmeta_df, lat='latitude', lon='longitude', radius=10,\n#                         center=dict(lat=47.6, lon=-122.3), zoom=7,\n#                         mapbox_style=\"stamen-terrain\")\n# fig.update_layout(title_text=\"Distribution of Bird Sightings\")\n# fig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.5\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">EBird Taxonomy</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"train_species_df = pd.read_csv(\"/kaggle/input/birdclef-2023/eBird_Taxonomy_v2021.csv\")\ntrain_species_df.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of the taxonomic order counts\nfig1 = px.histogram(train_species_df, x=\"TAXON_ORDER\", color_discrete_sequence=['aquamarine'])\nfig1.update_layout(title_text=\"Distribution of Taxonomic Orders\")\n\n# Bar plot of the species group counts\n# Box plot of the taxonomic order counts by category\nfig3 = px.box(train_species_df, x=\"CATEGORY\", y=\"TAXON_ORDER\", color_discrete_sequence=['red'])\nfig3.update_layout(title_text=\"Taxonomic Order Distribution by Category\")\n\n# Scatter plot of the taxonomic order counts by family\nfig4 = px.scatter(train_species_df, x=\"FAMILY\", y=\"TAXON_ORDER\")\nfig4.update_layout(title_text=\"Taxonomic Order Distribution by Family\")\n\n# Show the plots\nfig1.show()\nfig3.show()\nfig4.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n#  <center><font size = 3><span style=\"color:#a8dadc\">  <p style=\"background-color:#90e0ef;font-family:newtimeroman;color:#03045e;font-size:200%;text-align:center;border-radius:100px 10px;\">3. AUDIO EXPLORATION 🔉🔉 </p>   </span></font></center>\n\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> How to Visualize Audio Files ?? </b></span></font><br>\n\n<font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> There are many ways in which we can view audio in 2D like : </b></span></font>\n\n1. <font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> Waveforms :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">In audio processing, a waveform is a graphical representation of a sound signal that shows how the signal varies over time. It is a plot of the amplitude of the sound wave on the y-axis versus time on the x-axis. Waveforms can be used to visualize and analyze the properties of audio signals, such as frequency, amplitude, phase, and duration.</span></font><br>\n<img src =https://t4.ftcdn.net/jpg/03/27/36/95/360_F_327369570_CAxxxHHLvjk6IJ3wGi1kuW6WTtqjaMpc.jpg>\n\n2. <font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> Spectogram :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. Spectrum refers to plot/distribution of energy or amplitude as a function of frequency. Spectrogram is commonly used in signal processing, audio analysis, and other fields to analyze the “frequency content” of a time-varying signal.\nTo create a spectrogram, the signal is first divided into short segments (often using a technique called windowing). Then, for each segment, a mathematical transformation, such as the Fast Fourier Transform (FFT), is applied to convert the signal from the time domain to the frequency domain. The result is a set of frequency components and their respective amplitudes for that particular time segment.</span></font><br>\n<img src = https://i.stack.imgur.com/styEs.png>\n3. <font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b>  Mel Spectograms :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">The FFT (fast fourier transform) is computed on overlapping windowed segments of the signal, and we get what is called the spectrogram. A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale. The Mel Scale is a logarithmic transformation of a signal's frequency. The core idea of this transformation is that sounds of equal distance on the Mel Scale are perceived to be of equal distance to humans.</span></font><br>\n<img src =https://i.stack.imgur.com/8j108.png>\n4. <font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> Chromagram :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">The term chroma feature or chromagram closely relates to twelve different pitch classes. Chroma-based features, which are also referred to as \"pitch class profiles\", are a powerful tool for analyzing music whose pitches can be meaningfully categorized (often into twelve categories). A chromagram, is derived from the standard spectrogram, but instead of showing the energy distribution in frequency bins, it displays the energy distribution of musical pitch classes.\n</span></font><br>\n<img src = https://www.researchgate.net/profile/Giovanni-De-Poli/publication/265404164/figure/fig7/AS:295785980809229@1447532281700/Chromagram-of-a-framed-signal.png>\n\n5. <font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> MFCC :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC (mel-frequency cepstrum). They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale. The mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a “linear cosine transform” of a “log power spectrum” on a “nonlinear mel scale” of frequency. Further, cepstrum is derived from the spectrum of a signal by taking the inverse Fourier transform of the logarithm of the spectrum. It is essentially the spectrum of the spectrum. The term \"cepstrum\" comes from reversing the letters of the term \"spectrum\".</span></font><br>\n<img src =https://i.stack.imgur.com/nvBcU.png>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Helper Function</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"code","source":"def audio_eda(audio_path):\n   \n    # Load an audio file\n    samples, sample_rate = librosa.load(audio_path)\n\n    # Visualize the waveform\n    plt.figure(figsize=(14, 5))\n    librosa.display.waveshow(samples, sr=sample_rate)\n    plt.title('Waveform')\n\n    # Compute the spectrogram\n    spectrogram = librosa.stft(samples)\n    spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n\n    # Visualize the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(spectrogram_db, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram (dB)')\n\n    # Compute the mel spectrogram\n\n\n    # Visualize the mel spectrogram\n    S = librosa.feature.melspectrogram(y=samples, sr=sample_rate)\n\n    # Visualize mel spectrogram\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', fmax=8000, x_axis='time')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Mel spectrogram')\n    plt.tight_layout()\n\n\n    # Compute the chromagram\n    chromagram = librosa.feature.chroma_stft( y = samples , sr = sample_rate)\n\n    # Visualize the chromagram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(chromagram, sr=sample_rate, x_axis='time', y_axis='chroma')\n    plt.colorbar()\n    plt.title('Chromagram')\n\n    # Compute the MFCCs\n    mfccs = librosa.feature.mfcc(y=samples, sr=sample_rate, n_mfcc=13)\n\n    # Visualize the MFCCs\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')\n    plt.colorbar()\n    plt.title('MFCCs')\n\n    # Show the plots\n    display(Audio(samples, rate=sample_rate))\n    plt.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<p style=\"background-color:#03045e;font-family:newtimeroman;color:#90e0ef;font-size:140%;text-align:center;border-radius:200px 10px;\">Audio Exploration</p>\n\n#### [Top ↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n<p style=\"background-color:#f0f3bd;font-family:newtimeroman;color:#05668d;font-size:140%;text-align:center;border-radius:200px 10px;\">Black-and-white Mannikin</p>\n<img src = https://cdn.download.ams.birds.cornell.edu/api/v1/asset/212028741/1200>\n\n<font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> Primary Label :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">bawman1</span></font><br>\n\n<font size = 4><span style=\"color:#3A3E59;font-family:'Times New Roman'\"><b> Scientific Name :</b></span></font> <font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">Spermestes bicolor</span></font><br>\n\n#### [Top ↑](#top)\n","metadata":{}},{"cell_type":"code","source":"audio_eda(\"/kaggle/input/birdclef-2023/train_audio/bawman1/XC115075.ogg\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<center>\n<b><font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">Thank You for reading 😊  </span></font></b><br>\n<b><font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">Please do upvote if you liked the notebook   </span></font></b><br>\n<b><font size = 3><span style=\"color:#3A3E59;font-family:'Times New Roman'\">If you have any suggestions or feeback, please let me know  </span></font></b><br>\n  </center>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}